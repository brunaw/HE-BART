%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Galyardt at 2014-08-21 22:49:39 -0400 


%% Saved with string encoding Unicode (UTF-8) 

@article{Chipman1998,
abstract = {In this article we put forward a Bayesian approach for finding classification and regression tree (CART) models. The two basic components of this approach consist of prior specification and stochastic search. The basic idea is to have the prior induce a posterior distribution that will guide the stochastic search toward more promising CART models. As the search proceeds, such models can then be selected with a variety of criteria, such as posterior probability, marginal likelihood, residual sum of squares or misclassification rates. Examples are used to illustrate the potential superiority of this approach over alternative methods.},
author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
doi = {10.1080/01621459.1998.10473750},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Binary trees,Markov chain Monte Carlo,Mixture models,Model selection,Model uncertainty,Stochastic search},
title = {{Bayesian CART model search}},
year = {1998}
}

@article{Chipman2010,
abstract = {We develop a Bayesian "sum-of-trees" model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART's many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
archivePrefix = {arXiv},
arxivId = {0806.3286},
author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
doi = {10.1214/09-AOAS285},
eprint = {0806.3286},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Bayesian backfitting,Boosting,CART,Classification,Ensemble,MCMC,Nonparametric regression,Probit model,Random basis,Regularizatio,Sum-of-trees model,Variable selection,Weak learner},
title = {{BART: Bayesian additive regression trees}},
year = {2010}
}



  @Article{bartmachine,
    title = {{bartMachine}: Machine Learning with {B}ayesian Additive Regression Trees},
    author = {Adam Kapelner and Justin Bleich},
    journal = {Journal of Statistical Software},
    year = {2016},
    volume = {70},
    number = {4},
    pages = {1--40},
    doi = {10.18637/jss.v070.i04},
  }

@Manual{Rmanual,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2018},
    url = {https://www.R-project.org/},
  }

