<!DOCTYPE html>
<html>
  <head>
    <title>An Introduction to Bayesian Regression Trees</title>
    <meta charset="utf-8">
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/maynooth2.css" type="text/css" />
    <link rel="stylesheet" href="css/maynooth-fonts2.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# An Introduction to Bayesian Regression Trees
## Bruna Wundervald, Ph.D.Â Candidate in Statistics
### March, 2019

---





# Introduction 

Bayesian decision tree methods have experienced rapid development in recent years, being firstly proposed by Chipman, George, and McwCulloch (1998). 


`$$\Huge y = \sum_{j=1}^M g(X, T_j, \Theta_j, \mu_j) + \epsilon$$`

Chipman, Hugh A., Edward I. George, and Robert E. McCulloch. BART: Bayesian additive regression trees. _The Annals of Applied Statistics 4_, no. 1 (2010): 266-298.

---

# BCART

- The Bayesian CART model is a tree-based algorithm that allows us 
to introduce prior beliefs into the structure of the tree.

- Fundamentally differs from usual trees in the sense that it
posits a full probability model for the data, combining a prior 
distribution `\(P(\mathcal{T})\)` for the tree structure, with a likelihood
`\(P(\mathcal{D} | \mathcal{T})\)`, `\(D =\{(X_i, Y_i): 1 \leq i \leq N \}\)`. 

- Two main components:  a tree `\(\mathcal{T}\)` with `\(b\)`
terminal nodes, and a parameter set
`\(\Theta = (\theta_1,\dots,\theta_b)\)` that associates each parameter
`\(\theta_i\)` with the `\(i\)`th terminal node.

---
# BCART

The association of each individual observation with the terminal node
is made by denotting `\(y_{ij}\)` the `\(j\)`th observation of the `\(i\)`th node,
being `\(i = 1,\dots,b\)` and `\(j = 1,2,\dots,  n_i\)`. Define

`$$Y \equiv  (Y_1,...,Y_b)'$$`

and 
`$$Y_i \equiv (y_{i1}, \dots, y_{in_i})'$$`


and `\(X\)` and `\(X_i\)` are analogous. We typically assume that conditionally
on `\((\Theta, \mathcal{T})\)`, the values of `\(y\)` in a node are i.i.d., and 
across terminal nodes they are independent. The distribution for the
data is defined as 

`$$P(Y | X, \Theta, \mathcal{T}) = \prod_{i = 1}^{b}f(Y_i | \Theta_i) = \prod_{i = 1}^{b} \prod_{j = 1}^{n_i}f(y_{ij} | \theta_i)$$`


---
# BCART
  
  - Regression cases: `\(f(y_{ij} | \theta_i)\)` follows a Normal distribution. 

The mean shift model is set as

`$$y_{i1}, \dots, y_{in_i} | \theta_i \sim \mathcal{N}(\mu_i, \sigma^2),
\thinspace i = 1,\dots, b,$$`
  
and the mean-variance shift model is defined as 

`$$y_{i1}, \dots, y_{in_i} | \theta_i \sim \mathcal{N}(\mu_i, \sigma^2_i),
\thinspace i = 1,\dots, b$$`


---

# BCART
  
  Since the model is identified by `\((\Theta, \mathcal{T})\)`, we specify 
a prior probability distribution `\(P(\Theta, \mathcal{T})\)`. We 
conveniently use the relationship

$$P(\Theta, \mathcal{T}) = P(\Theta | \mathcal{T}) P(\mathcal{T}), $$
  
  and specify the two distributions separately.

- This strategy offers the advantage of choosing a prior to `\(\mathcal{T}\)` 
  that does not depend on the family of `\(\theta\)`. 


---
  
  # The form of `\(P(\mathcal{T})\)`
  
  -  Not exactly defined with a closed-form:
  determined by a tree-generating stochastic process, and each generation
  of this process (with MCMC) is considered a draw from this prior.  
  
The whole process is describe as:
  1. Setting a tree `\(\mathcal{T}\)` with a root node. 
  2. Split the terminal node `\(\eta\)` with probability 
  `\(P_{SPLIT}(\eta, \mathcal{T})\)`. 
  3. Split a rule `\(\rho\)` with probability 
  `\(P_{RULE}(\rho | \eta, \mathcal{T})\)`. Let 
  `\(\mathcal{T}\)` denote the new tree and repeat the process. 

`\(P_{SPLIT}(\eta, \mathcal{T})\)` is commonly
taken as `\(\alpha(1 - d_{\eta})^{b}\)`, where `\(d_{\eta}\)` denotes 
the depth of the `\(\eta\)` node. 
- the resulting prior `\(P(\mathcal{T})\)` will put higher
probabilities on 'bushy' trees.
---

  
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="img/psplit.png" alt="Different specifications for the probability of splitting a node (Chipman et al., 1998)" width="90%" height="90%" /&gt;
&lt;p class="caption"&gt;Different specifications for the probability of splitting a node (Chipman et al., 1998)&lt;/p&gt;
&lt;/div&gt;


---
  
  For the entire tree, we have that

`$$P(\mathcal{T}) = \prod_{\eta \in \mathcal{H}\text{terminals}} (1 - P_{SPLIT}(\eta ))\prod_{\eta \in \mathcal{H}\text{internals}} 
P_{SPLIT}(\eta ) 
\prod_{\eta \in \mathcal{H}\text{internals}} 
P_{RULE}(\rho | \eta )$$`
  
  where `\(\mathcal{H}\text{terminals}\)` represent the terminal nodes, while
the `\(\mathcal{H}\text{internals}\)` represent the internal nodes. So, for a 
new split, for example, this probability is: 
  
`$$P(\mathcal{T*}) = (1 -  P_{SPLIT}(\eta_L )) (1 - P_{SPLIT}(\eta_R )) P_{SPLIT}(\eta) P_{RULE}(\rho | \eta )$$`
  
where `\(\eta_L\)` and `\(\eta_R\)` represent the left and right node 
respectively, and they both have `\(d_{\eta_R}  = d_{\eta_L} = d_{\eta}  + 1\)`.

As for `\(P_{RULE}(\rho | \eta, \mathcal{T})\)`, it is usually
taken as a discrete uniform distribution over the range of 
values 'left' to be split on in each node and each variable. So
we can define 

`$$P_{RULE}(\rho | \eta, \mathcal{T}) = 
  \frac{1}{p_{adj}(\eta)} \frac{1}{n_{j.adj}(\eta)}$$`
  
  where `\(p_{adj}(\eta)\)` represents how many predictors are still
available to split in the node, and `\(n_{j.adj}(\eta)\)` how many
values in a given predictor are still available. 

---
## Parameter priors
  
  - Conjugate priors, assuming conditional independence between the nodes. 

Using the mean-shift model,
`\((\Theta, \mathcal{T})\)` has a simple prior with a standard
conjugate form 

`$$\mu_1, \dots, \mu_b | \sigma^2, \mathcal{T} 
\sim \mathcal{N}(0, \sigma^2_{\mu})$$`
  
  considering that `\(y\)` was scaled beforehand  and that
`\(\sigma_\mu = \frac{y_{max}}{2}\)`, which will give us
`\(\mu_\mu + 2 \sigma_\mu =  y_{max}\)`. The prior for the
dispersion of `\(y\)` is set as 

$$ \tau = 1/\sigma^2_y \sim Gamma(\nu/2, \lambda\nu/2),$$
  
  with the parameters `\(\lambda\)` and `\(\nu\)` being found in order to give
high probabilities of the BCART model improving an ordinary least 
squares model. 

---
## Parameter posteriors
  
`\begin{align}
P(\mu_i | X, Y, \mathcal{T}, \tau) &amp; \propto
P(Y| X, \mathbf{\mu}, \tau, \mathcal{T}) 
P(\mathbf{\mu} | \sigma^2_\mu, \mathcal{T}) \\
&amp; \propto \Big [ \prod_{j = 1}^{n_i} \mathcal{N}(y_{ij} | \mu_i, \tau)
                \Big] \mathcal{N}(\mu_i | \sigma^2_\mu, \mathcal{T}) \\
&amp; \propto   exp \Big\{-\frac{\tau}{2}
  \sum_{j = 1}^{n_i} (y_{ij} - \mu_i)^{2}
  \Big \} exp \Big\{-\frac{\sigma^{-2}_{\mu} \mu_i^{2} }{2}
    \Big \} \\
&amp; \propto exp \{ - Q/2 \}
\end{align}`

where 

`\begin{align}
Q 
&amp; \propto
(\sigma^{-2}_{\mu} + n_i \tau) \Big [ \mu_i -
                                       \frac{\tau \sum_{j = 1}^{n_i} y_{ij}}{(\sigma^{-2}_{\mu} + n_i \tau) }
                                     \Big]^{2}
\end{align}`


giving us that the posterior distribution for `\(\mu_i\)` is 
`\(\mathcal{N}\Big(\frac{\tau \sum_{j = 1}^{n_i} y_{ij}}{(\sigma^{-2}_{\mu} + n_i \tau)}, (\sigma^{-2}_{\mu} + n_i \tau)^{-1}\Big)\)`.

---
## Parameter posteriors

The posterior for `\(\tau = 1/\sigma^2_y\)` is found as 


`\begin{align}
P(\tau | X, Y, \mathcal{T}, \mathbf{\mu}) &amp; \propto
P(Y| X, \mathbf{\mu}, \tau, \mathcal{T}) P(\tau) \\
&amp; \propto
\tau^{\frac{\nu + n}{2} - 1} exp \Big\{
-\frac{\tau}{2} \Big(
\sum_{i = 1}^{b} \sum_{j = 1}^{n_i}(y_{ij} - \mu_i)^2 + 
\nu \lambda \Big) \Big\}
\end{align}`

giving us a final posterior as 

`$$\tau = 1/\sigma^2_y \sim G \Big(\frac{\nu + n}{2}, \frac{\tau}{2} \Big(\sum_{i = 1}^{b} \sum_{j = 1}^{n_i}(y_{ij} - \mu_i)^2 + \nu \lambda \Big) \Big)$$`

---
## Parameter posteriors

The conditional posterior distribution of `\(\mathcal{T}\)` is given by

`\begin{align}
P(\mathcal{T} | X, Y) &amp; \propto
P(\mathcal{T}) \int \int   P(Y| X, \mathbf{\mu}, \tau, \mathcal{T}) 
P(\mathbf{\mu} | \tau \mathcal{T}) P(\tau) d\tau d\mathbf{\mu} \\
&amp; \propto P(\mathcal{T}) P(Y| X, \mathcal{T}) 
\end{align}`

&gt; MCMC to sample from the posterior. 

---

# Possible actions

The transition kernel of the MCMC generates new trees
(denoted by `\(\mathcal{T*}\)`) by randomly choosing among 4 steps:

- **Grow**: Split a terminal node into two new ones. The node is
picked randomly accordingly to the `\(P_{RULE}\)`.  
- **Prune**: Prune a parent of two terminal nodes, by collapsing it.  
- Change: Randomly reassign the splitting rule of a terminal node. 
- Swap: Randomly pick a parent-child pair that are both internal nodes. 
Swap their splitting rules, unless the other child has an identical rule. 

*Grow and prune are the most usual steps. 

---

# Sampling algorithm

&lt;img src="img/algorithm.png" width="110%" height="110%" style="display: block; margin: auto;" /&gt;

where the probability of transition actually can be factorized in 

`$$r =\underbrace{\frac{P(\mathcal{T*} \rightarrow \mathcal{T})}{P(\mathcal{T} \rightarrow \mathcal{T*})}}_{\text{Transition ratio}}\underbrace{\frac{P(Y | \mathcal{T*}, \tau )}{(Y | \mathcal{T}, \tau )}}_{\text{Likelihood ratio}} \underbrace{\frac{P(\mathcal{T*})}{P(\mathcal{T}) }}_{\text{Tree structure ratio}}$$` 

---

## For a grow proposal

`$$P(\mathcal{T*} \rightarrow \mathcal{T}) = p(GROW)  \frac{1}{b} \frac{1}{p_{adj(\eta)}} \frac{1}{n_{j.adj(\eta)}}$$`

where b is the number of terminal nodes, `\(p_{adj(\eta)}\)` denotes the
number of predictors left available to split  and `\(n_{j.adj(\eta)}\)`
the number of values left to split in the picked variable. 

`$$P(\mathcal{T} \rightarrow \mathcal{T*}) = p(PRUNE)  P(\text{selecting } \eta \text{ to prune from}),$$`


`$$\frac{P(\mathcal{T*})}{P(\mathcal{T}) } = 
\frac{(1 -  P_{SPLIT}(\eta_L )) (1 - P_{SPLIT}(\eta_R )) P_{SPLIT}(\eta) P_{RULE}(\rho | \eta )}{(1 -  P_{SPLIT})},$$`

and 
`$$\frac{P(Y | \mathcal{T*}, \tau )}{(Y | \mathcal{T}, \tau )} = \sqrt{\frac{\tau^{-1}(\tau^{-1} + \sigma^2_{\mu})}{(\tau^{-1} +  n_{\mathcal{l}_L}\sigma^2_{\mu})(\tau^{-1} + n_{\mathcal{l}_R}\sigma^2_{\mu})}} \\ exp \Big\{\frac{\tau \sigma^2_{\mu} }{2}  \Big(\frac{(\sum_{i = 1}^{n_{\mathcal{l}_L}} y_{\mathcal{l}L, i})^2} {\tau^{-1} + n_{\mathcal{l}_L}\sigma^2_{\mu}} + \frac{(\sum_{i = 1}^{n_{\mathcal{l}_R}} y_{\mathcal{l}R, i})^2} {\tau^{-1} + n_{\mathcal{l}_R}\sigma^2_{\mu}} - \frac{(\sum_{i = 1}^{n_{\mathcal{l}}} y_{\mathcal{l}, i})^2} {\tau^{-1} + n_{\mathcal{l}}\sigma^2_{\mu}}\Big\}$$` 

where `\(n_{\mathcal{l}}\)` is the total of observations in the grown node, 
`\(n_{\mathcal{l}_L}\)` are the total in the resulting left node, and
`\(n_{\mathcal{l}_R}\)` is analogous.
---
    
# Gibbs sampling
    
![Hamiltonian Monte Carlo in action](https://raw.githubusercontent.com/chi-feng/mcmc-demo/master/docs/rwmh.gif)
    
---

# BART

BART can be considered a sum-of-trees ensemble.
Specifically, the model can be expressed as

`$$\mathbf{Y} = f(\mathbf{X}) + \mathbf{\epsilon} \approx \mathcal{T_1^{M}}(\mathbf{X}) + \dots + \mathcal{T_m^{M}}(\mathbf{X}) + \mathbf{\epsilon}, \mathbf{\epsilon} \sim \mathcal{N_n}(\mathbf{0}, \tau^{-1}\mathbf{I}_n)$$`

Here, we have `\(m\)` distinct regression trees, each composed of a tree 
structure, denoted by `\(\mathcal{T}\)`, and the parameters at the terminal 
nodes denoted by `\(M\)`. 

`$$P(\mathcal{T_1^{M}},\dots,\mathcal{T_m^{M}}, \tau ) = \Big[\prod_{t}\prod_{\mathcal{l}} P(\mu_{t, l} | \mathcal{T_t}) P(\mathcal{T_t}) \Big] P(\tau)$$`


---

# BART

A Metropolis-within-Gibbs sampler (Gelman and Gelman 1984; Hastings 1970)
is employed to generate draws from the posterior distribution of `\(P(\mathcal{T_1^{M}},\dots,\mathcal{T_m^{M}}, \tau | \mathbf{y})\)`.

The key of this sampler is to employ a form of âBayesian backfittingâ 
(Hastie and Tibshirani 2000) where the `\(j\)`th tree is fit iteratively, 
holding all other `\(m â 1\)` trees constant by exposing only the residual 
response that remains unfitted:

`$$\mathbf{R_{-j}} := \mathbf{y} - \sum_{t \neq j} \mathcal{T}_t^{M}(\mathbf{X})$$`

---

# BART works

A commonly used method for simulating complex regression-type data is:

`$$y = \beta_1 \sin(\pi x_1 x_2) + \beta_2 (x_3 - 0.5)^2 + \beta_3 x_4 + \beta_5 x_5 + \epsilon; \; \epsilon \sim N(0, 1)$$`

where `\(x_j \sim N(0, 1)\)` for `\(j=1, \ldots, 5.\)`

From the (Friedman, 1991) paper, called Multivariate Adaptive 
Regression Splines. 

Task: create good predictions of `\(y\)` and identify which variables are 
important. 
---

# BART works


&lt;img src="img/Friedman_benchmark.jpg" width="80%" height="100%" style="display: block; margin: auto;" /&gt;


---

class: big

# Key BART advantages

- Very good prediction properties.

- Deals with interactions and non-linear relationships.

- Based on a probability distribution so 'easily' extendable.

- A Bayesian model: have probabilistic uncertainty intervals for any required quantities. 

---

# BART prediction intervals

&lt;img src="img/BART_prediction_intervals.jpg" width="80%" height="100%" style="display: block; margin: auto;" /&gt;


---

# BART problems &amp; extensions

- The fitting algorithm is the main bottleneck. 
  - Not scalable

- Choosing split variables and values uniformly is very far from 
  optimal. 

- Better ways to explore trees?

- Lots of modeling types still not covered. 
  - Since BART is built on a standard set of probability distributions 
  it's quite easily extendable to other types of models *if you can still
  perform the collapsing over the terminal node parameters*


---
# References 


&lt;p&gt;&lt;cite&gt;Chipman, H. A, E. I. George and R. E. McCulloch
(1998).
&amp;ldquo;Bayesian CART model search&amp;rdquo;.
In: &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;.
ISSN: 1537274X.
DOI: &lt;a href="https://doi.org/10.1080/01621459.1998.10473750"&gt;10.1080/01621459.1998.10473750&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;&amp;mdash;
(2010).
&amp;ldquo;BART: Bayesian additive regression trees&amp;rdquo;.
In: &lt;em&gt;Annals of Applied Statistics&lt;/em&gt;.
ISSN: 19326157.
DOI: &lt;a href="https://doi.org/10.1214/09-AOAS285"&gt;10.1214/09-AOAS285&lt;/a&gt;.
eprint: 0806.3286.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;Kapelner, A. and J. Bleich
(2016).
&amp;ldquo;bartMachine: Machine Learning with Bayesian Additive Regression Trees&amp;rdquo;.
In: &lt;em&gt;Journal of Statistical Software&lt;/em&gt; 70.4, pp. 1&amp;ndash;40.
DOI: &lt;a href="https://doi.org/10.18637/jss.v070.i04"&gt;10.18637/jss.v070.i04&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;&lt;cite&gt;R Core Team
(2018).
&lt;em&gt;R: A Language and Environment for Statistical Computing&lt;/em&gt;.
R Foundation for Statistical Computing.
Vienna, Austria.
URL: &lt;a href="https://www.R-project.org/"&gt;https://www.R-project.org/&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;


---

class: center, middle, inverse

# Thanks!


&lt;img src= "https://s3.amazonaws.com/kleebtronics-media/img/icons/github-white.png", width="50", height="50",  align="middle"&gt; 

&lt;b&gt;

[@brunaw](https://github.com/brunaw)
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
