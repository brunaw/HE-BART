---
title: "Bayesian hierarchical modelling by hand"
author: 'Bruna Wundervald'
date: 'October, 2020'
output:
  rmarkdown::html_vignette:
    fig_width: 5
    fig_height: 3.5
    fig_cap: TRUE
    toc: yes
    css: style.css
header-includes:
  - \usepackage{mathtools}
  - \usepackage{setspace}
  - \usepackage[]{algorithm2e}
  - \usepackage{amsmath}
  - \usepackage{indentfirst}
editor_options: 
  chunk_output_type: console
---


<style type="text/css">
#TOC {
  margin: 0 270px;
  width: 425px;
}
</style>
</style>
<div class="outer">
<img src="./logo.png"  width="150px" display="block" align="bottom">
</div>
<b>
<center>
<a href="https://brunaw.com/"> Bruna Wundervald </a><br/>
<code>brunadaviesw at gmail.com</code><br/>
PhD Candidate in Statistics, Hamilton Institute
</center>
</b>
</div>
</div>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, 
                      warning = FALSE, fig.align = 'center', 
                      fig.height = 4.5, 
                      cache = FALSE, fig.pos='H')
```

# Model

Define the following model. Suppose we have data:
$$y_{ij}, i = 1,\ldots,n_j, \; j = 1\ldots, m$$
where $y_{ij}$ is observation $i$ in group $j$. There are different numbers of observations $n_j$ in each group, so for example $n_1$ is the number of observations in group 1, etc. There are $m$ groups. The total number of observations is $n = \sum_{j=1}^m n_j$

Suppose have a likelihood:
$$y_{ij} \sim N(\mu_j, \tau^{-1})$$

so each group has an overall mean $\mu_j$, with an overall precision term $\tau$. 

We then have a hierarchical prior distribution:

$$\mu_j \sim N(\mu, \omega^{-1})$$

and hyper-parameter prior distributions:

$$\mu \sim N(\mu_\mu, \tau_\mu)$$
$$\tau \sim Ga(\alpha, \beta)$$
$$\omega \sim Ga(\alpha_\omega, \beta_\omega)$$

Where the values $\mu_\mu, \tau_\mu, \alpha, \beta, \alpha_\omega, \beta_\omega$ are all fixed.



## Maths 

- Likelihood:

\begin{equation}
L = \prod_{j = 1}^{m} \prod_{i = 1}^{n_j} p(y_{ij} | \mu_{j}, \tau) \\
L \propto \tau^{n/2} exp \{ -\frac{\tau}{2} \sum_{j = 1}^{m}
\sum_{i = 1}^{n_j} (y_{ij} - \mu_j)^2 \}
\end{equation}

- $\mu_j | \mu, \omega$

\begin{equation}
p(\mu_1, \dots, \mu_m | \mu, \omega) \propto \omega^{m/2}
exp\{ - \frac{\omega}{2} \sum_{j = 1}^{m} (\mu_j - \mu)^2  \}
\end{equation}

- $\tau | \alpha, \beta$

$$p(\tau | \alpha, \beta) \propto \tau^{\alpha - 1} exp\{ - \beta \tau \}$$

- $\mu | \mu_{\mu}, \tau_{\mu}$

\begin{equation}
p(\mu | \mu_{\mu}, \tau_{\mu}) \propto \tau_{\mu}^{1/2}
exp\{ - \frac{ \tau_{\mu}}{2} (\mu - \mu_{\mu})^2  \}
\end{equation}


- $\omega$

$$p(\omega | \alpha_{\omega}, \beta_{\omega}) 
\propto \omega^{\alpha_{\omega} - 1} exp\{ - \beta_{\omega} \omega \}$$

- $p(\tau, \mu_1, \dots, \mu_m, \mu, \omega | y, \mu_{\mu}, 
\tau_{\mu}, \alpha, \beta, \alpha_{\omega},  \beta_{\omega})$


\begin{equation}
p(\tau, \mu_1, \dots, \mu_m, \mu, \omega | y, \mu_{\mu}, 
\tau_{\mu}, \alpha, \beta, \alpha_{\omega},  \beta_{\omega}) \propto

\tau^{\alpha - 1} exp\{ - \beta \tau \} \times 

\omega^{m/2}
exp\{ - \frac{\omega}{2} \sum_{j = 1}^{m} (\mu_j - \mu)^2  \} \\ 
\times  
\tau_{\mu}^{1/2}
exp\{ - \frac{ \tau_{\mu}}{2} (\mu - \mu_{\mu})^2  \} \times 
\omega^{\alpha_{\omega} - 1} exp\{ - \beta_{\omega} \omega \} \\
\times \tau^{n/2} exp \{ -\frac{\tau}{2} \sum_{j = 1}^{m}
\sum_{i = 1}^{n_j} (y_{ij} - \mu_j)^2 \}
\end{equation}


## Posterior for $\tau$ 

- $p(\tau | \mu_1, \dots, \mu_m, y, \alpha, \beta)$

\begin{equation}
p(\tau | \mu_1, \dots, \mu_m, y, \alpha, \beta) \propto
\tau^{\alpha - 1} exp\{ - \beta \tau \}  \times 
\tau^{n/2} exp \{ -\frac{\tau}{2} \sum_{j = 1}^{m}
\sum_{i = 1}^{n_j} (y_{ij} - \mu_j)^2 \} \\

\propto \tau^{n/2  + \alpha - 1 }
exp \{ - \tau \Big( \frac{\sum_{j = 1}^{m}
\sum_{i = 1}^{n_j} (y_{ij} - \mu_j)^2}{2} + \beta \Big) \}
\end{equation}

So  $\tau | \mu_1, \dots, \mu_m, y, \alpha, \beta \sim \text{Gamma}(n/2 + \alpha, 
\frac{\sum_{j = 1}^{m}
\sum_{i = 1}^{n_j} (y_{ij} - \mu_j)^2}{2} + \beta)$


## Posterior for $\mu_j$

- I know I don't have to use the joint distribution of 
$\mu_1, \dots, \mu_j$ for this but it seemed easier

\begin{equation}
Q =    \omega \sum_{j=1}^{m} (\mu_j - \mu)^2 + 
\tau \sum_{j = 1}^{m} \sum_{i = 1}^{n_j} (y_{ij} - \mu_j)^2 \\

Q =    \omega \sum_{j=1}^{m} (\mu_j^{2} - 2 \mu \mu_j + \mu^2) + 
\tau (\sum_{j = 1}^{m} (\sum_{i = 1}^{n_j} y_{ij}^{2} -
2 y_{ij} \mu_j)  + n_j \mu_j^2) \\

Q = \sum_{j=1}^{m} (n_j \tau + \omega) \mu_j^2 - 
\sum_{j = 1}^{m} (2 \mu \omega + 2 \bar y_j n_j \tau) \mu_j \\

Q = \sum_{j=1}^{m} [(n_j \tau + \omega) \mu_j^2 - 
2 \mu_j(\mu \omega +  \bar y_j n_j \tau) ] \\

Q \propto \sum_{j=1}^{m} [(n_j \tau + \omega)(\mu_j  - 
\frac{(\mu \omega +  \bar y_j n_j \tau)}{(n_j \tau + \omega)})^2 ] \\
\end{equation} 


So for each $\mu_j$


$$\mu_j | \mu, y, \tau, \omega \sim N( \frac{n_j \bar y_j \tau + \mu \omega}{n_j \tau + \omega}, (n_j \tau + \omega)^{-1})$$

## Posterior for $\mu$ 

Similarly, for $\mu$ we have: 

\begin{equation}
Q =    \omega \sum_{j = 1}^{m} (\mu_j - \mu)^2 +  \tau_{\mu}(\mu - \mu_{\mu})\\

Q =    \omega \sum_{j=1}^{m} (\mu_j^{2} - 2 \mu \mu_j + \mu^2) + 
\tau_{\mu} (\mu^2 - 2 \mu \mu_{\mu} +  \mu_{\mu}^2) \\

Q = (\tau_{\mu} + \omega m ) \mu^2 - 
2 \mu (\omega \bar \mu m + \mu_{\mu} \tau_{\mu}) \\

Q \propto (\tau_{\mu} + \omega m )(\mu - 
\frac{\omega \bar \mu m + \mu_{\mu} \tau_{\mu}}{\tau_{\mu} + \omega m})^2 \\
 \end{equation} 
 
 So for $\mu$ we have that: 


$$\mu | \mu_1, \dots, \mu_{m}, \mu_{\mu}, \tau_{\mu}, \omega \sim N(\frac{\omega \bar \mu m + \mu_{\mu} \tau_{\mu}}{\tau_{\mu} + \omega m} , (\tau_{\mu} + \omega m)^{-1})$$

## Posterior for $\omega$ 

\begin{equation}
p(\omega | \mu_1, \dots, \mu_m, \alpha_{\omega}, 
\beta{\omega}) \propto
\omega^{\alpha_{\omega} - 1} exp\{ - \beta_{\omega} \omega
\}  \times 
\omega^{m/2} exp \{ -\frac{\omega}{2} \sum_{j = 1}^{m}
(\mu_{j} - \mu)^2 \} \\

\propto \omega^{m/2  + \alpha_{\omega} - 1 }
exp \{ - \omega \Big( \frac{\sum_{j = 1}^{m}
(\mu_{j} - \mu)^2}{2} + \beta_{\omega} \Big) \}
\end{equation}

So  $\omega | \mu_1, \dots, \mu_m, \alpha_{\omega}, 
\beta{\omega} \sim \text{Gamma}(m/2 + \alpha_{\omega}, 
\frac{\sum_{j = 1}^{m}(\mu_{j} - \mu)^2}{2} + \beta)$

## JAGS code

First we establish the maths involved in this model and simulate from it: 

```{r}
library(R2jags)
library(tidyverse)

# Maths --------------------------------------------------------------

# Description of the Bayesian model fitted in this file
# Notation:
# y_{ij} = response variable for observation i = 1,...,n_j 
# in group j = 1,..,M.
# N = total number of observation = sum_j(n_j)
# mu_j = mean for each group j
# mu = mean for mu_j 
# tau = inverse variance of y_ij
# omega = inverse variance of mu_j

# Likelihood:
# y_{ij} ~ N(mu_j, tau^{-1})
# Priors
# mu_j ~ N(mu, omega^{-1})
# mu ~ N(mu_mu, tau_mu) 
# tau ~ Gamma(alpha, beta)
# omega ~ Gamma(alpha_omega, beta_omega)
# alpha = 0.5, beta = 1, alpha_omega = 0.2, beta_omega = 1, 
# mu_mu = 0, tau_mu = 0.5 

# Simulate data -----------------------------------------------------
alpha = 0.5; beta = 1; alpha_omega = 0.2; beta_omega = 1; 
mu_mu = 0; tau_mu = 0.5

# Set the seed so this is repeatable
set.seed(123)
# Some R code to simulate data from the above model
M = 4 # Number of groups
mu = rnorm(n = 1, mu_mu, sd = sqrt(1/tau_mu))
tau = rgamma(n = 1, 1/alpha, beta)
omega = rgamma(n = 1, 1/alpha_omega, beta_omega)
mu_j = rnorm(n = M, mu, sd = sqrt(1/omega))

nj = sample(200:350, M, replace = TRUE) # Set the number of obs in each group between 5 and 10
N = sum(nj)
group = rep(1:M, times = nj)
mu = rep(mu_j, nj)
y = rnorm(N, mean = mu, sd = sqrt(1/tau))

data.frame(y = y, group = group, 
           muj = mu) %>% 
  ggplot(aes(y = y, x = group, group = group)) +
  geom_boxplot(fill = "#CD5C5C", alpha = 0.8) +
  geom_point(aes(y = mu), colour = '#0066ff', size = 3) +
  theme_bw(18)
```

And then we run JAGS code and use the results to plot the posterior means in the plot: 

```{r}
# Jags code --------------------------------------------------------
# Jags code to fit the model to the simulated data

model_code = '
model
{
  # Likelihood
  for (i in 1:N) {
    y[i] ~ dnorm(mu_j[group[i]], tau^-1)
  }
  # Priors
  mu ~ dnorm(0, 1/0.5)
  omega ~ dgamma(0.2, 1)
  tau ~ dgamma(0.5, 1)
  for (j in 1:M) {
    mu_j[j] ~ dnorm(mu, omega^-1)
  }
}
'

# Set up the data
model_data = list(N = N, y = y, M = M, group = group)

# Choose the parameters to watch
model_parameters =  c("mu", "omega", "tau", "mu_j")

# Run the model
model_run = jags(data = model_data,
                 parameters.to.save = model_parameters,
                 model.file=textConnection(model_code))

# Simulated results -------------------------------------------------------

# Check the output - are the true values inside the 95% CI?
# Also look at the R-hat values - they need to be close to 1 if convergence has been achieved
# plot(model_run)
print(model_run)
traceplot(model_run)

# Create a plot of the posterior mean regression line
mu_j_pos = as.numeric(model_run$BUGSoutput$mean$mu_j) 
mu_j_pos = rep(mu_j_pos, nj)

data.frame(y = y, group = group, 
           muj = mu, mu_j_pos = mu_j_pos) %>% 
  ggplot(aes(y = y, x = group, group = group)) +
  geom_boxplot(fill = "#CD5C5C", alpha = 0.8) +
  geom_point(aes(y = mu), colour = '#0066ff', size = 3) +
  geom_point(aes(y = mu_j_pos), colour = '#ffff1a', size = 2) +
  ggtitle(label = "Blue points: true mean \nYellow points: posterior mean") +
  theme_bw(16)
```

## Gibbs algorithm


************************************************
**Algorithm 1**:  GIBBS Sampling

**Data**: Target variable $y$, groups $j = 1,\dots,m$

**Result**: Posterior distributions for all parameters

************************************************
**Initialisation**; 

Hyper-parameters values for $\mu_{\mu}, \tau_{\mu}, \alpha, \beta, \alpha_{\omega}, \beta{\omega}$;

Number of groups $n$;

Number of observations $n=\sum_{j = 1}^{j}n_j$; 

Number of iterations I; 

- **for** i from 1 to I **do**:
  - sample $\mu$, $\tau$, and $\omega$ from their priors
  - **for** j in 1:m **do**:
      - sample $\mu_j$ from its prior
  - **end**
  - calculate the log full conditional density of $y$ using the sampled values as $l_{new}$
  - calculate the log full conditional density of $y$ using the previous sampled values as $l_{old}$
  - set $a = exp(l_{new} - l_{old})$
  - generate $u \sim U(0, 1)$
  - **if** $a > u$ **then**:
    - accept the sampled values
    **else**
    - set i+1 = i (?)
  - **end**
  - get the posterior distributions for $\mu_j$, $\mu$, $\tau$, $\omega$
- **end**
  



******

